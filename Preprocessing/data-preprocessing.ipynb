{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Vikhy\n",
    "<br>\n",
    "Date: 22 March, 2022"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "1. Remove URLs\n",
    "2. Remove mentions\n",
    "3. Replace hashtags\n",
    "4. Replace emoticons with the sentiment\n",
    "5. Remove special characters\n",
    "6. Remove multilple spaces\n",
    "7. Remove stop words\n",
    "8. Correct spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../Datasets/SemEval6A.csv\")\n",
    "\n",
    "target = data[\"Target\"]\n",
    "tweets = data[\"Tweet\"]\n",
    "stance = data[\"Stance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       dear lord thank u for all of ur blessings forg...\n",
      "1       Blessed are the peacemakers, for they shall be...\n",
      "2       I am not conformed to this world. I am transfo...\n",
      "3       Salah should be prayed with #focus and #unders...\n",
      "4       And stay in your houses and do not display you...\n",
      "                              ...                        \n",
      "3261    \"Ok, he doesn't look like a mexican anymore bu...\n",
      "3262    \"Look around this crowd\\nWe are young, old,\\nW...\n",
      "3263    \"King Charles' slavery research is not even be...\n",
      "3264    \"In The Passion Of The Christ 2, Jesus gets ra...\n",
      "3265    \"Congratulations #12yearsaslave Go to Africa o...\n",
      "Name: Tweet, Length: 3266, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove URLs\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_urls(tweet):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub('', tweet)\n",
    "\n",
    "tweets = tweets.apply(remove_urls)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\vikhy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You are quite mistaken. :)   #SemST\n"
     ]
    }
   ],
   "source": [
    "# Remove mentions\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"words\")\n",
    "english_dictionary = set(nltk.corpus.words.words())\n",
    "\n",
    "def remove_mentions(tweet):\n",
    "    words = tweet.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if (word.startswith(\"@\") or word.startswith(\".@\") or word.startswith(\"/@\")) and word[1:] not in english_dictionary:\n",
    "            words[i] = \"\"\n",
    "        elif (word.startswith(\"@\") or word.startswith(\".@\") or word.startswith(\"/@\")) and word[1:] in english_dictionary:\n",
    "            words[i] = word[1:]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "tweets = tweets.apply(remove_mentions)\n",
    "print(tweets[86])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       dear lord thank u for all of ur blessings forg...\n",
      "1       Blessed are the peacemakers, for they shall be...\n",
      "2       I am not conformed to this world. I am transfo...\n",
      "3       Salah should be prayed with focus and understa...\n",
      "4       And stay in your houses and do not display you...\n",
      "                              ...                        \n",
      "3261    \"Ok, he doesn't look like a mexican anymore bu...\n",
      "3262    \"Look around this crowd We are young, old, We ...\n",
      "3263    \"King Charles' slavery research is not even be...\n",
      "3264    \"In The Passion Of The Christ 2, Jesus gets ra...\n",
      "3265    \"Congratulations 12yearsaslave Go to Africa or...\n",
      "Name: Tweet, Length: 3266, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Replace hastags with text\n",
    "\n",
    "def replace_hashtags(tweet):\n",
    "    words = tweet.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if \"#\" in word:\n",
    "            words[i] = word.replace(\"#\", \"\")\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "tweets = tweets.apply(replace_hashtags)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scorching summers for UK by end of the century! Thank Rocks I won't be here to suffer them! happy SemST\n"
     ]
    }
   ],
   "source": [
    "# Replace emoticons with the sentiment\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "emoticon_sentiment = {\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "emoticon_sentiment = {\n",
    "    \"ğŸ˜€\": \" happy \",\n",
    "    \"ğŸ˜\": \" happy \",\n",
    "    \"ğŸ˜‚\": \" laughing \",\n",
    "    \"ğŸ¤£\": \" laughing \",\n",
    "    \"ğŸ˜ƒ\": \" happy \",\n",
    "    \"ğŸ˜„\": \" happy \",\n",
    "    \"ğŸ˜…\": \" awkward \",\n",
    "    \"ğŸ˜†\": \" laughing \",\n",
    "    \"ğŸ˜‡\": \" blessed \",\n",
    "    \"ğŸ˜‰\": \" playful \",\n",
    "    \"ğŸ˜Š\": \" happy \",\n",
    "    \"ğŸ˜‹\": \" tasty \",\n",
    "    \"ğŸ˜Œ\": \" peaceful \",\n",
    "    \"ğŸ˜\": \" love \",\n",
    "    \"ğŸ˜\": \" cool \",\n",
    "    \"ğŸ˜\": \" smirking \",\n",
    "    \"ğŸ˜\": \" neutral \",\n",
    "    \"ğŸ˜’\": \" unhappy \",\n",
    "    \"ğŸ˜“\": \" sad \",\n",
    "    \"ğŸ˜”\": \" sad \",\n",
    "    \"ğŸ˜•\": \" confused \",\n",
    "    \"ğŸ˜–\": \" unhappy \",\n",
    "    \"ğŸ˜—\": \" kissing \",\n",
    "    \"ğŸ˜˜\": \" kissing \",\n",
    "    \"ğŸ˜™\": \" kissing \",\n",
    "    \"ğŸ˜š\": \" kissing \",\n",
    "    \"ğŸ˜›\": \" playful \",\n",
    "    \"ğŸ˜œ\": \" playful \",\n",
    "    \"ğŸ˜\": \" playful \",\n",
    "    \"ğŸ˜\": \" sad \",\n",
    "    \"ğŸ˜Ÿ\": \" worried \",\n",
    "    \"ğŸ˜ \": \" angry \",\n",
    "    \"ğŸ˜¡\": \" angry \",\n",
    "    \"ğŸ˜¢\": \" crying \",\n",
    "    \"ğŸ˜£\": \" unhappy \",\n",
    "    \"ğŸ˜¤\": \" angry \",\n",
    "    \"ğŸ˜¥\": \" sad \",\n",
    "    \"ğŸ˜¦\": \" worried \",\n",
    "    \"ğŸ˜§\": \" worried \",\n",
    "    \"ğŸ˜¨\": \" scared \",\n",
    "    \"ğŸ˜©\": \" unhappy \",\n",
    "    \"ğŸ˜ª\": \" tired \",\n",
    "    \"ğŸ˜«\": \" unhappy \",\n",
    "    \"ğŸ˜­\": \" crying \",\n",
    "    \"ğŸ˜®\": \" surprised \",\n",
    "    \"ğŸ˜¯\": \" surprised \",\n",
    "    \"ğŸ˜°\": \" worried \",\n",
    "    \"ğŸ˜±\": \" scared \",\n",
    "    \"ğŸ˜²\": \" surprised \",\n",
    "    \"ğŸ˜³\": \" shocked \",\n",
    "    \"ğŸ˜´\": \" sleepy \",\n",
    "    \"ğŸ˜µ\": \" shocked \",\n",
    "    \"ğŸ˜¶\": \" neutral \",\n",
    "    \"ğŸ¤¢\": \" nauseated \",\n",
    "    \"ğŸ¥µ\": \" hot \",\n",
    "    \"ğŸ¥¶\": \" cold \",\n",
    "    \"ğŸ¥´\": \" woozy \",\n",
    "    \"ğŸ¥º\": \" pleading \",\n",
    "    \"ğŸ¤«\": \" quiet \",\n",
    "    \"ğŸ¤¬\": \" cursing \",\n",
    "    \"ğŸ§\": \" inquiring \",\n",
    "    \"ğŸ¤¨\": \" skeptical \",\n",
    "    \":)\": \" happy \",\n",
    "    \":-)\": \"happy\",\n",
    "    \";)\": \" wink \",\n",
    "    \";-)\": \" wink \",\n",
    "    \":')\": \" laughing \",\n",
    "    \":/\": \" sad \",\n",
    "    \"^_^\": \" blessed \",\n",
    "    \";)\": \" joyful \",\n",
    "    \":p\": \" tasty \",\n",
    "    \":|]\": \" peaceful \",\n",
    "    \"<3\": \" love \",\n",
    "    \"B-)\": \" cool \",\n",
    "    \":^)\": \" smirking \",\n",
    "    \":|\": \" neutral \",\n",
    "    \":'(\": \" sad \",\n",
    "    \":S\": \" confused \",\n",
    "    \":*\": \" kiss \",\n",
    "    \":P\": \" joyful \",\n",
    "    \";P\": \" joyful \",\n",
    "    \"X-P\": \" joyful \",\n",
    "    \">:-( \": \" sad \",\n",
    "    \":O\": \" surprised \",\n",
    "    \":-O\": \" shocked \",\n",
    "    \"zzz\": \" sleepy \",\n",
    "    \":&\": \" nauseated \",\n",
    "    \":-/\": \" sad \",\n",
    "    \":'( \": \" sad \",\n",
    "    \":-@\": \" cursing \",\n",
    "    \"O_o\": \" shocked \",\n",
    "    \"\\U0001F600\": \" happy \",\n",
    "    \"\\U0001F601\": \" happy \",\n",
    "    \"\\U0001F602\": \" laughing \",\n",
    "    \"\\U0001F923\": \" laughing \",\n",
    "    \"\\U0001F603\": \" happy \",\n",
    "    \"\\U0001F604\": \" happy \",\n",
    "    \"\\U0001F605\": \" awkward \",\n",
    "    \"\\U0001F606\": \" laughing \",\n",
    "    \"\\U0001F607\": \" blessed \",\n",
    "    \"\\U0001F609\": \" playful \",\n",
    "    \"\\U0001F60A\": \" happy \",\n",
    "    \"\\U0001F60B\": \" tasty \",\n",
    "    \"\\U0001F60C\": \" peaceful \",\n",
    "    \"\\U0001F60D\": \" love \",\n",
    "    \"\\U0001F60E\": \" cool \",\n",
    "    \"\\U0001F60F\": \" smirking \",\n",
    "    \"\\U0001F610\": \" neutral \",\n",
    "    \"\\U0001F611\": \" unhappy \",\n",
    "    \"\\U0001F612\": \" sad \",\n",
    "    \"\\U0001F613\": \" sad \",\n",
    "    \"\\U0001F614\": \" confused \",\n",
    "    \"\\U0001F615\": \" unhappy \",\n",
    "    \"\\U0001F617\": \" kissing \",\n",
    "    \"\\U0001F618\": \" kissing \",\n",
    "    \"\\U0001F619\": \" kissing \",\n",
    "    \"\\U0001F61A\": \" kissing \",\n",
    "    \"\\U0001F61B\": \" playful \",\n",
    "    \"\\U0001F61C\": \" playful \",\n",
    "    \"\\U0001F61D\": \" playful \",\n",
    "    \"\\U0001F61E\": \" sad \",\n",
    "    \"\\U0001F61F\": \" worried \",\n",
    "    \"\\U0001F620\": \" angry \",\n",
    "    \"\\U0001F621\": \" angry \",\n",
    "    \"\\U0001F622\": \" crying \",\n",
    "    \"\\U0001F623\": \" unhappy \",\n",
    "    \"\\U0001F624\": \" angry \",\n",
    "    \"\\U0001F625\": \" sad \",\n",
    "    \"\\U0001F626\": \" worried \",\n",
    "    \"\\U0001F627\": \" worried \",\n",
    "    \"\\U0001F628\": \" scared \",\n",
    "    \"\\U0001F629\": \" unhappy \",\n",
    "    \"\\U0001F62A\": \" tired \",\n",
    "    \"\\U0001F62B\": \" unhappy \",\n",
    "    \"\\U0001F62D\": \" crying \",\n",
    "    \"\\U0001F62E\": \" surprised \",\n",
    "    \"\\U0001F62F\": \" surprised \",\n",
    "    \"\\U0001F630\": \" worried \",\n",
    "    \"\\U0001F631\": \" scared \",\n",
    "    \"\\U0001F632\": \" surprised \",\n",
    "    \"\\U0001F633\": \" shocked \",\n",
    "    \"\\U0001F634\": \" sleepy \",\n",
    "    \"\\U0001F635\": \" shocked \",\n",
    "    \"\\U0001F636\": \" neutral \",\n",
    "    \"\\U0001F92E\": \" nauseated \",\n",
    "    \"\\U0001F975\": \" hot \",\n",
    "    \"\\U0001F976\": \" cold \",\n",
    "    \"\\U0001F974\": \" woozy \",\n",
    "    \"\\U0001F97A\": \" pleading \",\n",
    "    \"\\U0001F92B\": \" quiet \",\n",
    "    \"\\U0001F92C\": \" cursing \",\n",
    "    \"\\U0001F9D0\": \" inquiring \"\n",
    "}\n",
    "\n",
    "\n",
    "def replace_emoticons(tweet):\n",
    "    for emoticon, sentiment in emoticon_sentiment.items():\n",
    "        tweet = tweet.replace(emoticon, sentiment)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "tweets = tweets.apply(replace_emoticons)\n",
    "print(tweets[865])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "\n",
    "def remove_special_characters(tweet):\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", tweet)\n",
    "\n",
    "\n",
    "tweets = tweets.apply(remove_special_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove multiple spaces\n",
    "\n",
    "def remove_multiple_spaces(tweet):\n",
    "    return re.sub(\"\\s+\", \" \", tweet)\n",
    "\n",
    "\n",
    "tweets = tweets.apply(remove_multiple_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "\n",
    "# <Note: It is not very helpful to remove stopwords considering the models. >\n",
    "\n",
    "# import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punkt\")\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# def remove_stop_words(tweet):\n",
    "#     words = word_tokenize(tweet)\n",
    "#     filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "#     return \" \".join(filtered_words)\n",
    "\n",
    "# stop_words = set(stopwords.words(\"english\"))\n",
    "# tweets = tweets.apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vikhy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       dear lord thank u for all of ur blessings forg...\n",
      "1       Blessed are the peacemakers for they shall be ...\n",
      "2       I am not conformed to this world I am transfor...\n",
      "3       salad should be prayed with focus and understa...\n",
      "4       And stay in your houses and do not display our...\n",
      "                              ...                        \n",
      "3261    Ok he doesn't look like a mexican anymore but ...\n",
      "3262    Look around this crowd We are young old We are...\n",
      "3263    King Charles slavery research is not even bett...\n",
      "3264    In The Passion Of The Christ 2 Jesus gets rape...\n",
      "3265    Congratulations 12yearsaslave Go to Africa or ...\n",
      "Name: Tweet, Length: 3266, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Correct incorrect spellings\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def correct_spellings(tweet):\n",
    "    words = word_tokenize(tweet)\n",
    "    corrected_words = [sc.correction(word) if sc.correction(word) else word for word in words]\n",
    "    corrected_tweet = \" \".join(corrected_words)\n",
    "    return corrected_tweet\n",
    "\n",
    "sc = SpellChecker()\n",
    "tweets = tweets.apply(correct_spellings)\n",
    "print(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data (Takes a long time to correct spellings)\n",
    "\n",
    "data = pd.concat([target, tweets, stance], axis=1)\n",
    "data.to_csv(\"../Datasets/corrected_tweets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       dear lord thank u for all of ur blessing forgi...\n",
      "1       Blessed are the peacemaker for they shall be c...\n",
      "2       I am not conformed to this world I am transfor...\n",
      "3       salad should be prayed with focus and understa...\n",
      "4       And stay in your house and do not display ours...\n",
      "                              ...                        \n",
      "3261    Ok he doe n't look like a mexican anymore but ...\n",
      "3262    Look around this crowd We are young old We are...\n",
      "3263    King Charles slavery research is not even bett...\n",
      "3264    In The Passion Of The Christ 2 Jesus get raped...\n",
      "3265    Congratulations 12yearsaslave Go to Africa or ...\n",
      "Name: Tweet, Length: 3266, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Read data again from the updated file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../Datasets/corrected_tweets.csv\")\n",
    "\n",
    "target = data[\"Target\"]\n",
    "tweets = data[\"Tweet\"]\n",
    "stance = data[\"Stance\"]\n",
    "\n",
    "print(tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "#<Note: Over aggressive pruning of words is observed. >\n",
    "\n",
    "# import nltk\n",
    "# from nltk.stem import SnowballStemmer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"wordnet\")\n",
    "\n",
    "# def stemming(tweet):\n",
    "#     tokens = word_tokenize(tweet)\n",
    "#     stemmer = SnowballStemmer(language=\"english\")\n",
    "#     stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "#     return \" \".join(stemmed_tokens)\n",
    "\n",
    "# tweets = tweets.apply(stemming)\n",
    "# print(tweets)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vikhy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vikhy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       dear lord thank u for all of ur blessing forgi...\n",
      "1       Blessed are the peacemaker for they shall be c...\n",
      "2       I am not conformed to this world I am transfor...\n",
      "3       salad should be prayed with focus and understa...\n",
      "4       And stay in your house and do not display ours...\n",
      "                              ...                        \n",
      "3261    Ok he doe n't look like a mexican anymore but ...\n",
      "3262    Look around this crowd We are young old We are...\n",
      "3263    King Charles slavery research is not even bett...\n",
      "3264    In The Passion Of The Christ 2 Jesus get raped...\n",
      "3265    Congratulations 12yearsaslave Go to Africa or ...\n",
      "Name: Tweet, Length: 3266, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "def lemmatization(tweet):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "\n",
    "tweets = tweets.apply(lemmatization)\n",
    "print(tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([target, tweets, stance], axis=1)\n",
    "data.to_csv(\"../Datasets/preprocessed_tweets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
